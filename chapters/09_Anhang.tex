\appendix
\chapter{}
\addcontentsline{toc}{chapter}{Anhang A}
\section{Script Neural Style Transfer}
\label{sec:script_neural_style_transfer}
Script zur Durchführung des Neural Style Transfer Algorithmus.

\begin{listing}[H]
\begin{minted}{python}
from tqdm import tqdm, tqdm_notebook
import matplotlib.pyplot as plt
import datetime

import torch
import torch.optim as optim
import torch.nn as nn

import torchvision.models as models

from csfnst.utils import load_image, plot_image_tensor, save_image_tensor
from csfnst.utils import rename_network_layers, replace_network_layers
from csfnst.losses import PerceptualLoss
\end{minted}
\captionof{lstlisting}{Imports}
\label{lst:neural_style_transfer_1}
\end{listing}

\begin{listing}[H]
\begin{minted}{python}
output_image_file = f'...'
style_image_file = '...'
content_image_file = '...'

USE_LBFGS = False
EPOCHS = 150

force_cpu = True
use_random_noise = True
content_image_size = 200, style_image_size = 200
\end{minted}
\captionof{lstlisting}{Einstellungen}
\label{lst:neural_style_transfer_2}
\end{listing}

\begin{listing}[H]
\begin{minted}{python}
device = torch.device('cuda' if torch.cuda.is_available() and not force_cpu else 'cpu')

style_image = load_image(style_image_file, size=style_image_size, normalize=False).to(device)
content_image = load_image(content_image_file, size=content_image_size, normalize=False).to(device)

if use_random_noise:
    output_image = torch.rand(content_image.shape[0], content_image.shape[1], content_image.shape[2]).to(device)
else:
    output_image = content_image.clone().to(device)
\end{minted}
\captionof{lstlisting}{Bilder laden}
\label{lst:neural_style_transfer_3}
\end{listing}


\begin{listing}[H]
\begin{minted}{python}
loss_network = rename_network_layers(
    replace_network_layers(
        models.vgg16(pretrained=True).features.to(device).eval()[:23],
        'MaxPool2d',
        'AvgPool2d'
    )
).to(device).eval()

criterion = PerceptualLoss(
    model=loss_network,
    content_layers=['relu3_3'],
    style_layers=['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'],
    style_image=style_image,    
    content_weigH=10,
    style_weigH=1e7,
    total_variation_weigH=1e-5,
)

optimizer = optim.LBFGS([output_image]) if USE_LBFGS else optim.Adam([output_image], lr=1e-1)
\end{minted}
\captionof{lstlisting}{Loss und Optimizer}
\label{lst:neural_style_transfer_4}
\end{listing}

\pagebreak

\begin{listing}[H]
\begin{minted}{python}
content_image.unsqueeze_(0)

output_image.unsqueeze_(0)
output_image.requires_grad_()

progress_bar = tqdm_notebook(range(EPOCHS))

if USE_LBFGS:
    for epoch in progress_bar:
        def closure():
            output_image.data.clamp_(0, 1)
            optimizer.zero_grad()

            loss = criterion(output_image, content_image)
            loss.backward()

            progress_bar.set_description(f'Loss: {loss.item():,.2f}')

            return loss

        optimizer.step(closure)
else:
    for epoch in progress_bar:
        output_image.data.clamp_(0, 1)
        optimizer.zero_grad()

        loss = criterion(output_image, content_image)
        loss.backward()

        progress_bar.set_description(f'Loss: {loss.item():,.2f}')

        optimizer.step()


content_image.squeeze_()

output_image.detach_()
output_image.squeeze_()
output_image.data.clamp_(0, 1)
\end{minted}
\captionof{lstlisting}{Durchführung des Neural Style Transfer}
\label{lst:neural_style_transfer_5}
\end{listing}

\pagebreak

\begin{listing}[H]
\begin{minted}{python}
fig, axes = plt.subplots(1, 3)
fig.set_size_inches(18, 20)

plot_image_tensor(content_image, ax=axes[0])
plot_image_tensor(style_image, ax=axes[1])
plot_image_tensor(output_image, ax=axes[2])

save_image_tensor(output_image, output_image_file)
\end{minted}
\captionof{lstlisting}{Ergebnisse visualisieren}
\label{lst:neural_style_transfer_6}
\end{listing}

\pagebreak


\section{Transformer Net}
\label{sec:transformer_net_full}

\begin{listing}[H]
\begin{minted}{python}
class TransformerNet(nn.Module):
    def __init__(
            self,
            channel_multiplier=32,
            bottleneck_size=5,
            bottleneck_type=BottleneckType.RESIDUAL_BLOCK,
            expansion_factor=6,
            final_activation_fn='Hardtanh',
            intermediate_activation_fn='PReLU'
    ):
        super(TransformerNet, self).__init__()

        self.pad = nn.ReflectionPad2d(padding=20)

        self.down1 = ConvBlock(3, channel_multiplier, kernel_size=9, stride=1, activation_fn=intermediate_activation_fn)
        self.down2 = ConvBlock(channel_multiplier, channel_multiplier * 2, kernel_size=5, stride=2,
                               activation_fn=intermediate_activation_fn)
        self.down3 = ConvBlock(channel_multiplier * 2, channel_multiplier * 4, kernel_size=5, stride=2,
                               activation_fn=intermediate_activation_fn)
\end{minted}
\captionof{lstlisting}{Down-Sampling}
\label{lst:transformer_net_full_1}
\end{listing}

\pagebreak

\begin{listing}[H]
\begin{minted}{python}
if bottleneck_type == BottleneckType.RESIDUAL_BLOCK:
    self.bottleneck = nn.Sequential(*[
        ResidualBlock(channel_multiplier * 4, channel_multiplier * 4, activation_fn=intermediate_activation_fn)
        for _ in range(bottleneck_size)
    ])
elif bottleneck_type == BottleneckType.MOBILE_VERSION_ONE_BLOCK:
    self.bottleneck = nn.Sequential(*[
        MobileVersionOneBlock(channel_multiplier * 4, channel_multiplier * 4,
                              activation_fn1=intermediate_activation_fn,
                              activation_fn2=intermediate_activation_fn)
        for _ in range(bottleneck_size)
    ])
elif bottleneck_type == BottleneckType.MOBILE_VERSION_TWO_BLOCK:
    self.bottleneck = nn.Sequential(*[
        MobileVersionTwoBlock(channel_multiplier * 4, channel_multiplier * 4, expansion_factor=expansion_factor,
                              activation_fn1=intermediate_activation_fn,
                              activation_fn2=intermediate_activation_fn)
        for _ in range(bottleneck_size)
    ])
else:
    raise ValueError('Wrong value for bottleneck_type')
\end{minted}
\captionof{lstlisting}{Bottleneck}
\label{lst:transformer_net_full_2}
\end{listing}

\begin{listing}[H]
\begin{minted}{python}
self.up1 = UpSampleBlock(channel_multiplier * 4, channel_multiplier * 2, kernel_size=5, scale_factor=2,
                                 activation_fn=intermediate_activation_fn)
self.up2 = UpSampleBlock(channel_multiplier * 2, channel_multiplier, kernel_size=5, scale_factor=2,
                         activation_fn=intermediate_activation_fn)
self.up3 = ConvBlock(channel_multiplier, 3, kernel_size=9, activation_fn=final_activation_fn)
\end{minted}
\captionof{lstlisting}{Up-Sampling}
\label{lst:transformer_net_full_3}
\end{listing}

\pagebreak

\begin{listing}[H]
\begin{minted}{python}
def forward(self, x):
    x = self.pad(x)

    x = self.down1(x)
    x = self.down2(x)
    x = self.down3(x)

    x = self.bottleneck(x)

    x = self.up1(x)
    x = self.up2(x)
    x = self.up3(x)

    return x
\end{minted}
\captionof{lstlisting}{Forward-Pass}
\label{lst:transformer_net_full_4}
\end{listing}