\appendix
\chapter{}
\addcontentsline{toc}{chapter}{Anhang}

\section{Notebook Neural Style Transfer}
\label{sec:nootebook_neural_style_transfer}
\begin{longlisting}
\begin{minted}{python}
from tqdm import tqdm_notebook
import matplotlib.pyplot as plt
import datetime

import torch
import torch.optim as optim
import torch.nn as nn

import torchvision.models as models

from csfnst.utils import load_image, plot_image_tensor, save_image_tensor
from csfnst.utils import rename_network_layers, replace_network_layers, get_criterion
from csfnst.losses import PerceptualLoss


output_image_file = '../images/output/htw-test.jpg'
style_image_file = 'license-checked/the_scream.jpg'
content_image_file = '../images/content/htw-768x768.jpg'

use_lbfgs = True
force_cpu = True
use_random_noise = False
content_image_size = 128
style_image_size = 768
epochs = 10

device = torch.device('cuda' if torch.cuda.is_available() and not force_cpu else 'cpu')
content_image = load_image(content_image_file, size=content_image_size, normalize=False).to(device)
style_image = load_image(f'../images/style/{style_image_file}', size=style_image_size, normalize=False).to(device)

if use_random_noise:
    output_image = torch.rand(content_image.shape[0], content_image.shape[1], content_image.shape[2]).to(device)
else:
    output_image = content_image.clone().to(device)

config = {
    'loss_network': 'vgg16',
    'content_weight': 1,
    'style_weight': 1e8,
    'total_variation_weight': 1e-5,
    'style_image': style_image_file,
    'style_image_size': style_image_size,
    'content_layers': ['relu3_3'],
    'style_layers': ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3']
}

criterion = get_criterion(config, device='cpu' if force_cpu else 'cuda')
optimizer = optim.LBFGS([output_image]) if use_lbfgs else optim.Adam([output_image], lr=1e-1)


content_image.unsqueeze_(0)
output_image.unsqueeze_(0)
output_image.requires_grad_()

content_loss_history = []
style_loss_history = []
total_variation_loss_history = []
loss_history = []

progress_bar = tqdm_notebook(range(epochs))

if use_lbfgs:
    for epoch in progress_bar:
        def closure():
            output_image.data.clamp_(0, 1)
            optimizer.zero_grad()

            loss = criterion(output_image, content_image)
            loss.backward()
            
            content_loss_history.append(criterion.content_loss_val)
            style_loss_history.append(criterion.style_loss_val)
            total_variation_loss_history.append(criterion.total_variation_loss_val)
            loss_history.append(criterion.loss_val)

            progress_bar.set_description(f'Loss: {loss.item():,.2f}')

            return loss

        optimizer.step(closure)
else:
    for epoch in progress_bar:
        output_image.data.clamp_(0, 1)
        optimizer.zero_grad()

        loss = criterion(output_image, content_image)
        loss.backward()
        
        content_loss_history.append(criterion.content_loss_val)
        style_loss_history.append(criterion.style_loss_val)
        total_variation_loss_history.append(criterion.total_variation_loss_val)
        loss_history.append(criterion.loss_val)

        progress_bar.set_description(f'Loss: {loss.item():,.2f}')

        optimizer.step()


content_image.squeeze_()

output_image.detach_()
output_image.squeeze_()
output_image.data.clamp_(0, 1)

fig, axes = plt.subplots(1, 3)
fig.set_size_inches(18, 20)

plot_image_tensor(content_image, ax=axes[0])
plot_image_tensor(style_image, ax=axes[1])
plot_image_tensor(output_image, ax=axes[2])

save_image_tensor(output_image, output_image_file)


fig, axes = plt.subplots(1, 1, figsize=(18, 20))  
axes.plot(content_loss_history, label='Content Loss')
axes.plot(style_loss_history, label='Style Loss')
axes.plot(total_variation_loss_history, label='Total Variation Loss')
axes.plot(loss_history, label='Loss')
plt.legend()
plt.show()
\end{minted}
\captionof{lstlisting}{Notebook zur Durchführung des Neural Style Transfer Algorithmus}
\label{lst:notebook_neural_style_transfer}
\end{longlisting}


\section{Script Activation-Functions}
\label{sec:script_activation_functions}
\begin{longlisting}
\begin{minted}{python}
def get_activation_fn(name):
    activation_fn_map = {
        'ELU': lambda: nn.ELU(),
        'ReLU': lambda: nn.ReLU(),
        'RReLU': lambda: nn.RReLU(),
        'PReLU': lambda: nn.PReLU(),
        'SELU': lambda: nn.SELU(),
        'CELU': lambda: nn.CELU(),
        'ReLU6': lambda: nn.ReLU6(),
        'Hardtanh': lambda: nn.Hardtanh(min_val=0.0, max_val=1.0),
        'Sigmoid': lambda: nn.Sigmoid(),
        'None': lambda: None
    }

    return activation_fn_map[name]()
\end{minted}
\captionof{lstlisting}{Activation-Functions}
\label{lst:script_activation_functions}
\end{longlisting}

\section{Script ConvBlock}
\label{sec:script_conv_block}
\begin{longlisting}
\begin{minted}{python}
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, activation_fn='None'):
        super(ConvBlock, self).__init__()

        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride)
        self.norm = nn.InstanceNorm2d(out_channels, affine=True)
        self.activation_fn = get_activation_fn(activation_fn)

    def forward(self, x):
        x = self.norm(self.conv(x))
        x = self.activation_fn(x) if self.activation_fn else x

        return x
\end{minted}
\captionof{lstlisting}{ConvBlock}
\label{lst:script_conv_block}
\end{longlisting}

\section{Script ResidualBlock}
\label{sec:script_residual_block}
\begin{longlisting}
\begin{minted}{python}
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, inner_channels=None, kernel_size=3, activation_fn='None'):
        super(ResidualBlock, self).__init__()

        inner_channels = inner_channels if inner_channels else in_channels

        self.conv1 = nn.Conv2d(in_channels, inner_channels, kernel_size)
        self.conv2 = nn.Conv2d(inner_channels, out_channels, kernel_size)

        self.pad1 = nn.ReflectionPad2d(padding=kernel_size // 2)
        self.pad2 = nn.ReflectionPad2d(padding=kernel_size // 2)

        self.norm1 = nn.InstanceNorm2d(inner_channels, affine=True)
        self.norm2 = nn.InstanceNorm2d(out_channels, affine=True)

        self.activation_fn = get_activation_fn(activation_fn)

    def forward(self, x):
        identity = x

        x = self.norm1(self.conv1(self.pad1(x)))
        x = self.activation_fn(x) if self.activation_fn else x
        x = self.norm2(self.conv2(self.pad2(x)))

        return x + identity
\end{minted}
\captionof{lstlisting}{ResidualBlock}
\label{lst:script_residual_block}
\end{longlisting}

\section{Script UpsampleBlock}
\label{sec:script_upsample_block}
\begin{longlisting}
\begin{minted}{python}
class UpSampleBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, scale_factor=None,
                 activation_fn='None'):
        super(UpSampleBlock, self).__init__()

        self.scale_factor = scale_factor

        self.conv = ConvBlock(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            activation_fn=activation_fn
        )

    def forward(self, x):
        if self.scale_factor:
            x = F.interpolate(x, mode='nearest', scale_factor=self.scale_factor)

        return self.conv(x)
\end{minted}
\captionof{lstlisting}{UpsampleBlock}
\label{lst:script_upsample_block}
\end{longlisting}

\section{Script TransformerNet}
\label{sec:script_transformer_net}
\begin{longlisting}
\begin{minted}{python}
class TransformerNet(nn.Module):
    def __init__(self, channel_multiplier=32, bottleneck_size=5, bottleneck_type=BottleneckType.RESIDUAL_BLOCK,
                 expansion_factor=6, final_activation_fn='None', intermediate_activation_fn='None'):
        super(TransformerNet, self).__init__()

        self.pad = nn.ReflectionPad2d(padding=20)

        self.down1 = ConvBlock(3, channel_multiplier, kernel_size=9, stride=1, activation_fn=intermediate_activation_fn)
        self.down2 = ConvBlock(channel_multiplier, channel_multiplier * 2, kernel_size=5, stride=2,
                               activation_fn=intermediate_activation_fn)
        self.down3 = ConvBlock(channel_multiplier * 2, channel_multiplier * 4, kernel_size=5, stride=2,
                               activation_fn=intermediate_activation_fn)

        if bottleneck_type == BottleneckType.RESIDUAL_BLOCK:
            self.bottleneck = nn.Sequential(*[
                ResidualBlock(channel_multiplier * 4, channel_multiplier * 4, activation_fn=intermediate_activation_fn)
                for _ in range(bottleneck_size)
            ])
        else:
            raise ValueError('Wrong value for bottleneck_type')

        self.up1 = UpSampleBlock(channel_multiplier * 4, channel_multiplier * 2, kernel_size=5, scale_factor=2,
                                 activation_fn=intermediate_activation_fn)
        self.up2 = UpSampleBlock(channel_multiplier * 2, channel_multiplier, kernel_size=5, scale_factor=2,
                                 activation_fn=intermediate_activation_fn)
        self.up3 = ConvBlock(channel_multiplier, 3, kernel_size=9, activation_fn=final_activation_fn)

    def forward(self, x):
        x = self.pad(x)
        x = self.down3(self.down2(self.down1(x)))
        x = self.bottleneck(x)
        return self.up3(self.up2(self.up1(x)))
\end{minted}
\captionof{lstlisting}{TransformerNet}
\label{lst:script_transformer_net}
\end{longlisting}

\chapter{}

\section{Performanz-Test mit 1920 * 1080 (Full HD) Pixel Bildern}
\begin{table}[H]
    \centering
    \begin{tabular}{ |c|c|c|c|c| }
        \hline
        \textbf{Name} & \textbf{XPS 15 - CPU} & \textbf{XPS 15 - GPU} & \textbf{Jetson TX1 - CPU} & \textbf{Jetson TX1 - GPU}   \\ \hline
        Network 1 & 8.56369 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 2 & 3.43223 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 3 & 1.39450 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 4 & 7.55040 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 5 & 3.06514 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 6 & 1.30307 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 7 & 6.92937 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 8 & 2.84717 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 9 & 1.21508 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
    \end{tabular}
    \caption{Berechnungsgeschwindigkeit für Bilder der Größe 1920 * 1080 Pixel}
    \label{tab:1920x1080}
\end{table}

\section{Performanz-Test mit 1024 * 768 Pixel Bildern}
\begin{table}[H]
    \centering
    \begin{tabular}{ |c|c|c|c|c| }
        \hline
        \textbf{Name} & \textbf{XPS 15 - CPU} & \textbf{XPS 15 - GPU} & \textbf{Jetson TX1 - CPU} & \textbf{Jetson TX1 - GPU}   \\ \hline
        Network 1 & 2.84025 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 2 & 1.04201 sec & 0.04790 sec                            & & \\ \hline
        Network 3 & 0.48485 sec & 0.02463 sec                            & & \\ \hline
        Network 4 & 2.58202 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 5 & 0.96789 sec & 0.01090 sec                            & & \\ \hline
        Network 6 & 0.45279 sec & 0.00567 sec                            & & \\ \hline
        Network 7 & 2.35020 sec & \textcolor{danger}{nicht durchführbar} & & \\ \hline
        Network 8 & 0.89255 sec & 0.00414 sec                            & & \\ \hline
        Network 9 & 0.42105 sec & 0.00414 sec                            & & \\ \hline
    \end{tabular}
    \caption{Berechnungsgeschwindigkeit für Bilder der Größe 1024 * 768 Pixel}
    \label{tab:1024x768}
\end{table}

\section{Performanz-Test mit 640 * 480 Pixel Bildern}
\begin{table}[H]
    \centering
    \begin{tabular}{ |c|c|c|c|c| }
        \hline
        \textbf{Name} & \textbf{XPS 15 - CPU} & \textbf{XPS 15 - GPU} & \textbf{Jetson TX1 - CPU} & \textbf{Jetson TX1 - GPU}   \\ \hline
        Network 1 & 1.04294 sec & 0.04822 sec & & \\ \hline
        Network 2 & 0.41771 sec & 0.02446 sec & & \\ \hline
        Network 3 & 0.19684 sec & 0.00980 sec & & \\ \hline
        Network 4 & 0.96311 sec & 0.00955 sec & & \\ \hline
        Network 5 & 0.38778 sec & 0.00490 sec & & \\ \hline
        Network 6 & 0.18396 sec & 0.00485 sec & & \\ \hline
        Network 7 & 0.88547 sec & 0.00421 sec & & \\ \hline
        Network 8 & 0.35785 sec & 0.00419 sec & & \\ \hline
        Network 9 & 0.17067 sec & 0.00415 sec & & \\ \hline
    \end{tabular}
    \caption{Berechnungsgeschwindigkeit für Bilder der Größe 640 * 480 Pixel}
    \label{tab:640x480}
\end{table}










