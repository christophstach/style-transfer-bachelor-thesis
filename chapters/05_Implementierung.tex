\chapter{Implementierung}
\label{cha:implementation}

Im Bereich des Machine Learning und Deep Learning gibt es viele verschiedene Frameworks, die bei der Umsetzung und dem Training von Neuronalen Netzwerken behilflich sind. Beispiele hierfür sind Tensorflow\footnote{\url{https://www.tensorflow.org/}} von Google, PyTorch\footnote{\url{https://pytorch.org/}} von Facebook und Apache MXNet\footnote{\url{https://mxnet.apache.org/}}. Da bereits im Vorfeld dieser Arbeit Erfahrungen mit PyTorch gesammelt wurden, wurde dieses Framework ausgesucht.

\section{Loss-Funktion}

Im ersten Schritt wird die Loss-Funktion implementiert, die das Kernstück des modellbasierten und des optimierenden Ansatzes ist.
PyTorch beinhaltet bereits vortrainierte Versionen verschiedener Modellarchitekturen. In dieser Arbeit wird das \gls{vgg16}-Modell benutzt, auf dieses wird mit \mintinline{python}{model = torchvision.models.vgg16(pretrained=True).features} zugegriffen. Das PyTorch-Modell ist in die Bereiche \textit{features} und \textit{classifier} aufgeteilt. Es wird der Bereich \textit{features} benötigt, welcher die bereits vortrainierten Convolutional-Layer enthält. Der Bereich \textit{classifier} enthält die Fully-Connected-Layer des Modells, welche für die Aufgabenstellung ungenutzt bleiben.

\pagebreak

\subsection{Zugriff auf Hidden-Layer-Ergebnisse}

Um bei PyTorch auf Zwischenergebnisse der Hidden-Layer zuzugreifen, werden hinter diesen Hooks implementiert. Das kann in eine Funktion ausgelagert werden, welche die \gls{activation_map}s während der Berechnung des Forward-Pass durch das \gls{vgg16}-Modell extrahiert.

\begin{listing}[H]
\begin{minted}{python}
def extract_activation_maps(image_batch, model, layers, detach=False):
    class SaveActivationMap:
        def __init__(self):
            self.activation = []
        def hook(self, model, inpt, outpt):
            self.activation.append(outpt.detach() if detach else outpt)

    sam = SaveActivationMap()
    handles = []

    for layer in layers:
        i = list(dict(model.named_children()).keys()).index(str(layer))
        handles.append(model[i].register_forward_hook(sam.hook))
    model(image_batch)

    for handle in handles:
        handle.remove()

    return sam.activation
\end{minted}
\captionof{lstlisting}{Extrahierung der Activation-Maps mit PyTorch}
\end{listing}

\subsection{Content-Loss}

Um das Content-Loss zu bilden, wird mit der zuvor definierten Funktion auf die Zwischenergebnisse der Hidden-Layer zugegriffen.
Das Ergebnis ist das \gls{mse_loss} aller Zwischenergebnisse der Eingangsdaten (Inhaltsbild) verglichen mit den Zwischenergebnissen der Ausgangsdaten (generierte Bilder), vgl. Kapitel \ref{sec:content_loss}.

\begin{listing}[H]
\begin{minted}{python}
def content_loss(self, y, x):
    y = extract_activation_maps(y, self.model, self.c_layers)
    x = extract_activation_maps(x, self.model, self.c_layers, detach=True)
    return sum([
        F.mse_loss(generated_feature, input_feature)
        for generated_feature, input_feature in zip(y, x)
    ])
\end{minted}
\captionof{lstlisting}{Berechnung des Content-Loss, vgl. Gleichung \eqref{eq:content_loss}}
\end{listing}

\subsection{Style-Loss}

Für das Style-Loss wird eine Funktion benötigt, um die Gram-Matrix eines \gls{tensor}s zu bilden. Die Anzahl der Dimensionen des Eingangstensors wird auf zwei Dimensionen geflächt. Diese Matrix wird mit einer von sich selbst transponierten Version multipliziert. Danach wird das Ergebnis über das Produkt der Anzahl der Channel, Höhe und Breite des Eingangstensors normalisiert.

\begin{listing}[H]
\begin{minted}{python}
def gram_matrix(tensor):
    b, c, h, w = tensor.shape
    normalizer = c * h * w
    tensor_flat = tensor.flatten(2)

    return torch.div(
        torch.bmm(tensor_flat, tensor_flat.transpose(1, 2)),
        normalizer
    )
\end{minted}
\captionof{lstlisting}{Berechnung der Gram-Matrix, vgl. Gleichungen \eqref{eq:gram_matrix_1} u. \eqref{eq:gram_matrix_2}}
\end{listing}

Die Gram-Matrix-Funktion wird dazu benutzt, das Style-Loss zu berechnen. Wie beim Content-Loss wird das \gls{mse_loss} gebildet. Dazu werden vorher die Gram-Matrizen aller \gls{activation_map}s berechnet. Auf diese Weise wird das berechnete Ausgangsbild mit dem vorher eingestellten Stilbild verglichen. In dieser Implementierung wurde die Gewichtung der einzelnen Layer nicht berücksichtigt. Die Implementierung weicht daher geringfügig von der in den Grundlagen verwendeten Gleichung \eqref{eq:style_loss} ab.

\begin{listing}[H]
\begin{minted}{python}
def style_loss(self, y):
    y = extract_activation_maps(y, self.model, self.s_layers)
    y_grams = [gram_matrix(row) for row in y]

    return sum([
        F.mse_loss(y_gram, style_gram)
        for y_gram, style_gram in zip(y_grams, self.style_grams)
    ])
\end{minted}
\captionof{lstlisting}{Berechnung des Style-Loss, vgl. Gleichung \eqref{eq:style_loss}}
\end{listing}

\pagebreak

\subsection{Total-Variation-Loss}

Das Total-Variation-Loss berechnet die Summe der Abweichungen eines Pixels zum nächsten Pixel eines Bildes. Dies wird für Höhe und Breite des Bildes durchgeführt und addiert.

\begin{listing}[H]
\begin{minted}{python}
def total_variation_loss(self, y):
    return torch.add(
        torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])),
        torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))
    )
\end{minted}
\captionof{lstlisting}{Berechnung des Total-Variation-Loss \cite{pytorch-implementation-of-perceptual-losses-for-real-time-style-transfer}, vgl. Gleichung \eqref{eq:total_variation_loss}}
\end{listing}

\subsection{Perceptual-Loss}

Das Perceptual-Loss ist das Gesamt-Loss und addiert die einzelnen Teil-Losse mit entsprechend einstellbaren Gewichtungsfaktoren.

\begin{listing}[H]
\begin{minted}{python}
def forward(self, y, x):
    content_loss = self.c_weight * self.content_loss(y, x)
    style_loss = self.s_weight * self.style_loss(y)
    total_variation_loss = self.tv_weight * self.total_variation_loss(y)

    loss = content_loss + style_loss + total_variation_loss

    return loss
\end{minted}
\captionof{lstlisting}{Berechnung des gesamten Perceptual-Loss, vgl. Gleichung \eqref{eq:perceptual_loss}}
\end{listing}

\section{Neural Style Transfer}

Die Implementierung des Neural Style Transfer Algorithmus wird in einem Jupyter Notebook\footnote{\url{https://jupyter.org/}} durchgeführt. 
Dadurch können im späteren Verlauf unterschiedliche Tests durchgeführt werden, da die Hyperparameter schnell angepasst werden können.

Das Notebook, vgl. \ref{sec:nootebook_neural_style_transfer}, orientiert sich am zuvor erstellten Programmablaufplan, vgl. Abbildung \ref{img:neural_style_pap_img}. Es bietet außerdem die Auswahl zwischen den PyTorch-Optimizern \gls{adam} und L-BFGS \cite{Liu1989}. Folgender Code zeigt die Optimierung von Parametern mit PyTorch.

\begin{listing}[H]
\begin{minted}{python}
criterion = PerceptualLoss(...)
optimizer = optim.Adam([outputs])

for epoch in range(epochs):
    optimizer.zero_grad()

    loss = criterion(outputs, inputs)
    loss.backward()

    optimizer.step()    
\end{minted}
\captionof{lstlisting}{Vereinfachter Code einer Trainingsschleife in PyTorch}
\end{listing}

\section{Fast Neural Style Transfer}

Das Verfahren des Fast Neural Style funktioniert ähnlich wie beim vorher vorgestellten Neural Style Transfer. Anstatt der Pixel des Ausgangsbildes werden jedoch die Gewichte eines Neuronalen Netzwerks optimiert. Der bestehende Code kann aus  dem vorherigen Kapitel wiederverwendet werden. Lediglich die Initialisierung des PyTorch Optimizers muss angepasst werden.

\begin{listing}[H]
\begin{minted}{python}
model = TransformerNet(...)
optimizer = optim.Adam(model.parameters())
\end{minted}
\captionof{lstlisting}{Optimierung der Gewichte des Netzwerks}
\end{listing}

\subsection{COCO-Datensatz}

Um das Neuronale Netzwerk zu trainieren, wird ein großer Bilddatensatz benötigt. In dieser Arbeit wird dafür der \gls{coco}-Datensatz der Firma Microsoft verwendet \cite{DBLP:journals/corr/LinMBHPRDZ14}. Es handelt sich um einen öffentlichen, frei verfügbaren Datensatz vieler verschiedener Bilder unterschiedlicher Kategorien. Während des Trainingsverlaufs werden zufällige Bilder aus dem \gls{coco}-Datensatz genutzt und das Loss  (Style-, Content- und Total-Variation-Loss) über die Ausgaben, die das Netzwerk generiert, berechnet. Mit dem berechneten Loss ist PyTorch in der Lage, die Gradienten der Gewichte des Neuronalen Netzwerks zu berechnen und diese schrittweise zu optimieren.

Der \gls{coco}-Datensatz liegt in Form vieler verschiedener \gls{jpeg}-Bilder vor. Um nicht alle Bilder gleichzeitig laden zu müssen, bietet PyTorch die Möglichkeit, einen Dataloader\footnote{Weitere Information zum PyTorch-Dataloader: \url{https://pytorch.org/tutorials/beginner/data_loading_tutorial.html}} zu nutzen, der die Bilder in der gewünschten Batchgröße Schritt für Schritt lädt.

\begin{listing}[H]
\begin{minted}{python}
dataset = datasets.ImageFolder(
    config['dataset_path'],
    transform=transforms.Compose([
        transforms.Resize(config['content_image_size']),
        transforms.CenterCrop(config['content_image_size']),
        transforms.ToTensor()
    ])
)

dataloader = data.DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)
\end{minted}
\captionof{lstlisting}{Der PyTorch-Dataloader lädt Bilder in der gewünschten Batchgröße}
\end{listing}

Der Dataloader lädt alle Bilder in allen Unterordnern innerhalb des konfigurierten Pfades.

\subsection{Netzwerkarchitektur}
\label{sec:network_architecture}

Die Architektur des Neuronalen Netzes spielt eine wichtige Rolle. Sie muss in der Lage sein, die Eigenschaften eines Stils möglichst gut zu erlernen und gleichzeitig performant bleiben. Daher wurde die in Kapitel \ref{sec:fast_neural_style_transfer} vorgestellte Netzwerkarchitektur des Image Transformer Networks implementiert. Dies geschieht in PyTorch in Form einer Klasse, welche von \mintinline{python}{Module} erbt.

Um in Kapitel \ref{cha:tests} das Testen unterschiedlicher Netzwerkarchitekturen zu ermöglichen, wird die Klasse des Image Transformer Networks möglichst dynamisch implementiert. Einstellbar sind folgende Parameter, die im weiteren Verlauf erklärt werden:

\begin{itemize}
    \item channel\_multiplier
    \item bottleneck\_size
    \item bottleneck\_type
    \item final\_activation\_fn
    \item intermediate\_activation\_fn
\end{itemize}

\subsection{Aktivierungsfunktionen}

Für den dynamischen Aufbau des Netzwerks wird eine Funktion benötigt, die entsprechende PyTorch-Aktivierungsfunktionen zurückgibt. Hardtanh und Sigmoid kommen als finale Aktivierungsfunktionen des Netzwerks in Betracht, da sie Werte zwischen $ 0.0 $ und $ 1.0 $ zurückliefern. Das Ergebnis eignet sich für die Generierung von Bildern, welche in PyTorch ebenfalls Pixelwerte zwischen $ 0.0 $ und $ 1.0 $ besitzen. Zu beachten ist, dass die Hardtanh-Funktion mit den Parametern \mintinline{python}{min_val=0.0} und \mintinline{python}{max_val=1.0} initialisiert wird, die den Wertebereich verschieben. Mit den Standardeinstellungen liegt der Wertebereich der Hardtanh-Funktion zwischen $ -1.0 $ und $ 1.0 $. Die Parameter \mintinline{python}{final_activation_fn} und \mintinline{python}{intermediate_activation_fn} werden auf die entsprechenden PyTorch-Aktivierungsfunktionen überführt.

\subsection{ConvBlock}

Des weiteren müssen die in Kapitel \ref{sec:aufbau} konzipierten Layer-Blöcke implementiert werden. Der ConvBlock besteht aus den PyTorch-Layern \mintinline{python}{Conv2d}, \mintinline{python}{InstanceNorm2d} und der gewählten Aktivierungsfunktion. \mintinline{python}{InstanceNorm2d} wurde aufgrund des Papers \cite{DBLP:journals/corr/UlyanovVL16} gewählt und dort näher in Bezug auf die Verwendung mit Style Transfer-Methoden erläutert.

Die Parameter \mintinline{python}{in_channels} und \mintinline{python}{out_channels} ergeben sich aus dem Parameter \mintinline{python}{channel_multiplier} ($ m $) multipliziert mit einem festen Wert, der je nach Layer die Werte $ 1 $, $ 2 $ oder $ 4 $ beträgt.

\subsection{ResidualBlock}

Der ResidualBlock, vgl. \ref{img:residual_block_img}, besteht aus zwei aufeinanderfolgenden \mintinline{python}{Conv2d}-Layern. Wie beim vorherigen ConvBlock werden die Daten über \mintinline{python}{InstanceNorm2d} normalisiert. Abschließend wird die Aktivierungsfunktion angewendet. Die Besonderheit ist, dass die Eingangsdaten kopiert und danach mit den berechneten Daten der Convolutional-Layer addiert werden. Durch \mintinline{python}{Conv2d} verkleinert sich die Dimension der Daten, vgl. \ref{sec:conv_networks}, was durch den \mintinline{python}{ReflectionPad2d}-Layer wieder ausgeglichen wird. Eingangsdaten und berechnete Daten müssen die gleichen Dimensionen aufweisen, um addiert werden zu können. Die Parameter \mintinline{python}{in_channels} und \mintinline{python}{out_channels} ergeben sich aus \mintinline{python}{channel_multiplier} multipliziert mit $ 4 $.

\subsection{UpSampleBlock}

Der UpSampleBlock schaltet dem ConvBlock ein Upsampling-Verfahren vor, um das Bild um einen entsprechenden Faktor zu vergrößern. In dieser Arbeit wird Nearest Neighbor Interpolation verwendet.

\subsection{TransformerNet}

Neben den Layer-Blöcken wurde noch die eigentliche Netzwerkarchitektur implementiert. Die Implementierung nimmt die in \ref{sec:network_architecture} beschriebenen Parameter im Konstruktor entgegen, um eine entsprechende Netzwerkarchitektur zu generieren. Neben den bereits beschriebenen Einstellungen gibt \mintinline{python}{bottleneck_size} die Anzahl der hintereinander geschalteten ResidualBlocks an. Der \mintinline{python}{bottleneck_type} ist für zukünftige Erweiterungen vorgesehen, damit der ResidualBlock durch einen anders aufgebauten Block ersetzt werden kann. Außerdem wurde ein anfängliches Padding eingefügt, um die ursprüngliche Bildgröße beizubehalten.

\subsection{Weitere Implementierungen}

Zusätzlich zu den vorgestellten Implementierungen wurden Hilfsskripte und Jupyter Notebooks zum Testen erstellt. Außerdem wurde eine Trainer-Klasse entworfen, die in der Lage ist, Einstellungen für Trainingsläufe aus einer Konfigurationsdatei zu lesen und somit 
ein aufeinanderfolgendes Training mehrerer Modelle zu ermöglichen. Auf diese Art kann man viele unterschiedliche Parametereinstellungen testen.

Zur Realisierung eines Prototyps wurde eine JSON-API mit dem Framework Flask\footnote{Flask: \url{http://flask.pocoo.org/}} erstellt. Die API stellt einen Endpunkt zur Verfügung, welcher den Style Transfer mit einem gewünschten Modell durchführt. Sie wurde in einen Docker\footnote{Docker: \url{https://www.docker.com/}}-Container überführt, um das Hosting auf unterschiedlichen Geräten zu erleichtern.

Um die Ergebnisse zu visualisieren, wurde eine Frontend-Applikation mit dem Framework Angular\footnote{Angular: \url{https://angular.io/}} erstellt. Diese kann Bilder über die Webcam eines Handys oder eines Computers aufnehmen und sie an die Flask-API senden, die nachfolgend das aufgenommene Bild in einen vorher ausgewählten Stil überführt.

Skripte, Frontend-Applikation und JSON-API sind nicht Bestandteil dieser Arbeit und befinden sich in einem prototypischen Zustand. Der komplette Code zu diesem Projekt ist unter \url{https://github.com/christophstach/style-transfer-project} öffentlich einsehbar. Der Code des Frontends unter \url{https://github.com/christophstach/style-transfer-frontend}. Ein gehosteter Prototyp zum Testen des Style Transfers ist unter \url{https://stylized.christophstach.me/} verfügbar.