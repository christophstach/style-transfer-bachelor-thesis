\chapter{Evaluation}

Im Kapitel Evaluation werden die restlichen Kapitel dieser Arbeit beleuchtet und kritisch betrachtet. Dabei wird auf die Kapitel Grundlagen \ref{cha:fundamentals}, Methodologie \ref{cha:methodology}, Implementierung \ref{cha:implementation} und Tests und Experimente \ref{cha:tests} eingangen.

\section{Grundlagen}

Im Kapitel Grundlagen \ref{cha:fundamentals} werden die Grundlagen zu Neuronalen Netzwerken erleutert. Besonders der Backprogation-Algorithmus \ref{sec:backpropagation} und die Beschreibung sowie tieferes Auseinandersetzen mit Convolutions \ref{sec:conv_networks} sind für das Verständniss der Problemstellung hilfreich.

Anhand der Paper von Gatys \cite{DBLP:journals/corr/GatysEB15a} und Johnson \cite{DBLP:journals/corr/JohnsonAL16} kann die Problemstellung des Style Transfer nachvollzogen werden. Die in den Papern verwedendte Loss-Funktion ist bei beiden die Gleiche. Jedoch werden unterschiedliche Notationen verwendet.
Die Loss-Funktion kann für das Trainieren eines Neuronalen Netzwerks wiederverwendet werden. Das Paper \cite{DBLP:journals/corr/JohnsonAL16} geht jedoch nicht auf die genaue Archtiktur des verwendenten Netzwerks ein. Hierzu ist es hilfreich die offizielle Implementierung \cite{Johnson2016} oder die Referenz-Implementierungen im PyTorch-Repository \cite{OnlineToturialNeuralStylePyTorch} zu analysieren.

Das Konzept wurde von Total-Variation-Denoising wurde in den Referenz-Implementierungen verwendet, ist dort jedoch unzureichend beschrieben. Eine detailierte Beschreibung ist in den Papern \cite{RUDIN1992259, DBLP:journals/corr/EstrelaMS16} vorhanden.

\section{Methodologie}

Im Kapitel \ref{cha:methodology} wird die Vorgehensweise der geplanten Implementierung beschrieben. Das bereits in \cite{DBLP:journals/corr/JohnsonAL16} beschriebene Konzept des Transformer Networks wird um die Parameter der Bottleneck-Size $ s $ und Channnel-Multiplikator $ m $ erweitert. Dadurch können beliebig viele Netzwerkarchitekturen dynamisch erstellt werden und es wird die Grundlage für weitreichende Performanz-Tests geschaffen.

Das Konzept des ResidualBlocks ist dahinhend intressant, da es auch für andere Problemstellung verwendet werden kann. Der Effekt des \gls{vanishing_gradient} kann mit ihnen bei jeglicher Art von Convolutional-Neural-Network verringert werden und ein Training dieser erleichtern.

\section{Implementierung}

Bei der Implementierungen der Netzwerkarchtiktur erwies es sich als vorteilhaft diese möglichst dynamisch aufzubauen. Die ursprünglich geplante, abschließende Aktivierungs-Funktion Hardtanh \ref{sec:hardtanh} konnte somit problemlos durch die Sigmoid-Aktivierungs-Funktion \ref{sec:sigmoid} ausgetauscht werden. Es wird außerdem die Möglichkeit vorgeplant die im Bottleneck-Teil des Netzwerk verwendenten ResidualBlocks, durch andere Arten von Blöcken auszutauschen.

Das in \ref{sec:method_neural_style_transfer} entworfene Skript zur Durchführung des Neural-Style-Algorithmus konnte zum Training der Netzwerke nicht wiederverwendet werden. Hierzu wird eine Trainer-Klasse entworfen die weitreichende Konfigurationsmöglichkeiten bietet und das aufeianderfolgende Training mehrer Netzwerke erleichtert.

\section{Tests und Experimente}

Im Kapitel Tests und Experimente \ref{cha:tests} werden verschiedene Tests mit verschiedenen Hyperparametereingestellungen durchgeführt.
Es wird demonstriert wie sich verschiedene Content-, Style- und Total-Variation-Loss-Gewichtungen zueinander verhalten. Style-Transfer im Allgemeinen hat weitere Hyperparameter, welche bei den durchgeführten Tests nicht berücktsichtigt wurden.

Außerdem werden verschiedene Tests hinsichtlich der Netzwerkarchitektur und Performanz durchgeführt. Es wird gezeigt, das auch Netzwerkarchitekturen mit weniger lernbaren Parameter in der Lage sein können Stile zu erlernen. Ein umfassender Test wird jedoch nur mit The Starry Night \cite{the_starry_night_img} durchgeführt.

Zudem wird gezeigt, dass die gewählten Netzwerkarchitekturen größtenteils auf Geräten mit leistungsarmer Hardware zu berechnen sind.